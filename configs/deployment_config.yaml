# Deployment Configuration

api:
  host: "0.0.0.0"
  port: 8000
  workers: 4
  reload: false
  log_level: "info"

inference:
  model_path: "models/checkpoints/best_model.pth"
  device: "cpu"  # Options: cpu, cuda
  batch_size: 1

  # Performance thresholds
  max_inference_time_ms: 500
  confidence_threshold: 0.7

monitoring:
  enable_metrics: true
  metrics_port: 9090

  # Alerts
  alert_on_high_latency: 1000  # ms
  alert_on_low_confidence: 0.5

security:
  enable_cors: true
  allowed_origins:
    - "http://localhost:3000"
    - "http://localhost:8080"
  max_request_size_mb: 10
  rate_limit: 100  # requests per minute
